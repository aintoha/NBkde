library(caret)
library(doParallel)

data <- iris
 
gbmTrain <- data[sample(nrow(data), round(nrow(data)*0.9),  replace = F),]

#Here I have randomly sampled the data to develop a training set. I have left 10% of the data as a test set to ensure that the model is not overfitting the training data.

#this creates the tuning grid, ensure you name the features the same as the hyper parameters. Hyperparameters are essentially the 'settings' of the algorithm
grid <- expand.grid(n.trees = c(1000,1500), interaction.depth=c(1:3), shrinkage=c(0.01,0.05,0.1), n.minobsinnode=c(20))


#This creates the train control. in this example I am using a repeated k-folds cross validation with k= 5 repeated 2 times, allowing parallel.
ctrl <- trainControl(method = "repeatedcv",number = 5, repeats = 2, allowParallel = T)
 
#Register parallel cores
registerDoParallel(detectCores()-1)

x = train[,1:4]
y = train[,5]

# set up 10-fold cross validation procedure
train_control <- trainControl(
  method = "cv", 
  number = 10
)

# set up tuning grid
search_grid <- expand.grid(
  usekernel = TRUE,
  fL = 0:5,
  adjust = seq(0, 5, by = 1)
)


#build model
set.seed(124) #for reproducability
unwantedoutput <- capture.output(GBMModel <- train(x = x, y = y,
                  method = "nb", trControl = train_control,
                  tuneGrid = search_grid))

#Note that the "capture.output" function has been used here to avoid pages of output being displayed in the vignette, making it unreadable.

print(GBMModel)