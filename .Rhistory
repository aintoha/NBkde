traindata = data.frame(data$norm, data$class)
traindata
sort(traindata)
order(traindata)
traindata[order(data.class)]
traindata[order(data.class),]
traindata[,order(data.class)]
dplyr(arrange(traindata, data.class()))
dplyr::arrange(traindata, data.class)
predict_nbkde = function(traindata, testdata, h) {
n_x = ncol(traindata) # no. of column in test data
n_variable = ncol(as.matrix(traindata[, -n_x]))  # no. of features
n_test = nrow(testdata) # no. of instances in testdata
n_train = nrow(traindata)
sort_traindata = dplyr::arrange(traindata, traindata[,n_x])
n_class = length(unique(traindata[,n_x])) # no. of class
class_name = unique(sort_traindata[, n_x]) #name of the class
prior = rep(NA, n_class)
for (k in 1:n_class) {
class = noquote(as.character(unique(traindata[,n_x])))
prior[k] = length(which(traindata[, n_x]  == class[k]))/n_train
}
length_prior = length(prior) # no. of prior
if (length_prior != n_class) {
stop("prior must have the same length as the number or class")
}
if (sum(prior) != 1) {
stop("sum of prior must equal to 1")
}
if (is.null(testdata)) {
testdata = traindata # the covariates
} else {
if (is.matrix(testdata) | is.data.frame(testdata)) {
testdata = as.data.frame(testdata)
}
else {
stop("testdata must be a matrix or data frame")
}
}
if (is.null(traindata)) {
stop("No train data")
} else {
if (is.matrix(traindata) | is.data.frame(traindata)) {
traindata = as.data.frame(traindata)
}
else {
stop("traindata must be a matrix or data frame")
}
}
subset_traindata = subset.data(traindata)
if (is.null(h)) {
bw = 2^seq(from=-4, to= 2, by=0.1)
ind = list()
train = list()
test = list()
training_data = list()
testing_data = list()
loss = array(NA, dim=c(length(bw),  n_variable, n_class))
h = matrix(NA, ncol = n_class, nrow = n_variable)
for (k in 1:n_class) {
ind[[k]] <- sample(2, nrow(subset_traindata[[k]]),
replace = T, prob = c(0.8, 0.2))
train[[k]] <- subset_traindata[[k]][ind[[k]] == 1,]
test[[k]] <- subset_traindata[[k]][ind[[k]] == 2,]
for (w in 1:length(bw)) {
for (j in 1:n_variable) {
training_data[[k]] = train[[k]][,-n_x]
testing_data[[k]] = test[[k]][,-n_x]
loss[w, j, k] = logloss(h = bw[w], traindata = as.matrix(training_data[[k]])[,j],
testdata= as.matrix(testing_data[[k]])[,j])
}
}
}
for (k in 1:n_class) {
for (w in 1:length(bw)) {
for (j in 1:n_variable) {
h[j, k] =  bw[which(loss[, j, k] == min(loss[,j , k]))]
}
}
}
} else if (is.numeric(h)) {
if (length(h) != n_variable * n_class) {
stop("Number of bandwidth is invalid")
} else {
h = matrix(h, byrow = T, ncol = n_class)
}
} else if (is.matrix(h)) {
if (nrow(h) != n_variable) {
stop("Number of row for bandwidth is invalid. Number of column must equal to number of class")
}
if (ncol(h) != n_class) {
h = matrix(rep(h, n_class), ncol= n_class)
}
} else if (is.matrix(h)) {
if (nrow(h) == n_variable) {
if (ncol(h) == n_class) {
h =h
}
}
}
#   n_h = length(h) # no.of of bandwidth
#
#   h_matrix = matrix(h, ncol = n_variable)
#   # no. of column is the number of class
#   # no. of row is the number of variable
#
#   if (n_h == 1) {
#
#     h = matrix(rep(h, (n_variable * n_class)), ncol = n_class,nrow = n_variable, )
#
#   } else if (nrow(h_matrix) == 1) {
#
#   h = matrix(rep(h_matrix, n_class), ncol= n_class)
#
#   } else if (nrow(h_matrix) != n_class)  {
#
#   stop("Number of bandwidth is invalid")
#
#   } else if (is.numeric(h))  {
#
#   h = matrix(h, byrow = T, ncol = n_class )
#
#   } else {
#
#   h = h
#   }
# }
# h is a matrix of bandwidth with (C x P) dimension:
# C: no of class
# P: no of variable
data = list()
pdf = list()
for(k in 1:n_class) {
data[[k]]= subset_traindata[[k]][, -n_x]
pdf [[k]] = multiple.pdf(traindata = data[[k]],
testdata = testdata, h = h[,k])
}
# prior =  unlist(prior)
# return (c(pdf, prior))
prod_pdf = list()
for (k in 1:n_class) {
prod_pdf[[k]] = data.frame(unlist(pdf[[k]])) %>% mutate(Prod = Reduce(`*`, .))
}
prob_pdf = matrix(NA, nrow = n_test, ncol = n_class)
for (k in 1:n_class) {
prob_pdf[, k]  = prod_pdf[[k]]$Prod *  prior[k]
# this the probability for each pdf
# the row is the number of test point
# the column the number of class
}
sum_prob = as.matrix(apply(prob_pdf, 1, sum), byrow = T, nrow = n_test)
p_class = matrix(NA, nrow = n_test, ncol = n_class)
for (k in 1:n_class) {
for (i in 1:n_test) {
p_class[i,k ] = prob_pdf[i ,k]/sum_prob[i,]
}
}
p_class = as.data.frame(p_class)
colnames(p_class) <- class_name
h_matrix = h
# no. of column is number of class
# no. of row is number of variables
colnames(h_matrix) <- class_name
rownames(h_matrix) <- names(traindata[,-n_x])
result = list("conditional probability" = p_class,
"prior probability" = prior,
"bandwidth"= h_matrix)
return(result)
}
#######################
traindata = PlantGrowth[1:25,]
testdata = PlantGrowth[26:30,]
model <- naive_bayes(group ~ ., traindata,
usekernel = TRUE, kernel = "gaussian")
n <- 100
set.seed(1)
data <- data.frame(class = sample(c("classA", "classB"), n, TRUE),
bern = sample(LETTERS[1:2], n, TRUE),
cat  = sample(letters[1:3], n, TRUE),
logical = sample(c(TRUE,FALSE), n, TRUE),
norm = rnorm(n),
count = rpois(n, lambda = c(5,15)))
traindata = data.frame(data$norm, data$class)
ind <- sample(2, nrow(traindata), replace = T, prob = c(0.95, 0.05))
train_data <- traindata[ind == 1,]
test_data <- traindata[ind == 2,]
model <- naive_bayes(data.class ~ ., data = train_data,
usekernel = TRUE, kernel = "gaussian")
model
plot(model)
p <- predict(model, test_data, type = 'prob')
p
#########################
traindata = train_data
testdata = as.matrix(test_data[,-2])
h = c(0.3586, 0.4197)
# h= NULL
pdf = predict_nbkde(traindata=traindata, testdata=testdata, h=h)
pdf
model
h = c(0.4197, 0.3586,)
h = c(0.4197, 0.3586)
# h= NULL
pdf = predict_nbkde(traindata=traindata, testdata=testdata, h=h)
pdf
p
model
n_x = ncol(traindata) # no. of column in test data
n_variable = ncol(as.matrix(traindata[, -n_x]))  # no. of features
n_test = nrow(testdata) # no. of instances in testdata
n_train = nrow(traindata)
sort_traindata = dplyr::arrange(traindata, traindata[,n_x])
n_class = length(unique(traindata[,n_x])) # no. of class
class_name = unique(sort_traindata[, n_x]) #name of the class
class_name
prior = rep(NA, n_class)
for (k in 1:n_class) {
class = noquote(as.character(unique(traindata[,n_x])))
prior[k] = length(which(traindata[, n_x]  == class[k]))/n_train
}
length_prior = length(prior) # no. of prior
if (length_prior != n_class) {
stop("prior must have the same length as the number or class")
}
if (sum(prior) != 1) {
stop("sum of prior must equal to 1")
}
if (is.null(testdata)) {
testdata = traindata # the covariates
} else {
if (is.matrix(testdata) | is.data.frame(testdata)) {
testdata = as.data.frame(testdata)
}
else {
stop("testdata must be a matrix or data frame")
}
}
if (is.null(traindata)) {
stop("No train data")
} else {
if (is.matrix(traindata) | is.data.frame(traindata)) {
traindata = as.data.frame(traindata)
}
else {
stop("traindata must be a matrix or data frame")
}
}
subset_traindata = subset.data(traindata)
if (is.null(h)) {
bw = 2^seq(from=-4, to= 2, by=0.1)
ind = list()
train = list()
test = list()
training_data = list()
testing_data = list()
loss = array(NA, dim=c(length(bw),  n_variable, n_class))
h = matrix(NA, ncol = n_class, nrow = n_variable)
for (k in 1:n_class) {
ind[[k]] <- sample(2, nrow(subset_traindata[[k]]),
replace = T, prob = c(0.8, 0.2))
train[[k]] <- subset_traindata[[k]][ind[[k]] == 1,]
test[[k]] <- subset_traindata[[k]][ind[[k]] == 2,]
for (w in 1:length(bw)) {
for (j in 1:n_variable) {
training_data[[k]] = train[[k]][,-n_x]
testing_data[[k]] = test[[k]][,-n_x]
loss[w, j, k] = logloss(h = bw[w], traindata = as.matrix(training_data[[k]])[,j],
testdata= as.matrix(testing_data[[k]])[,j])
}
}
}
for (k in 1:n_class) {
for (w in 1:length(bw)) {
for (j in 1:n_variable) {
h[j, k] =  bw[which(loss[, j, k] == min(loss[,j , k]))]
}
}
}
} else if (is.numeric(h)) {
if (length(h) != n_variable * n_class) {
stop("Number of bandwidth is invalid")
} else {
h = matrix(h, byrow = T, ncol = n_class)
}
} else if (is.matrix(h)) {
if (nrow(h) != n_variable) {
stop("Number of row for bandwidth is invalid. Number of column must equal to number of class")
}
if (ncol(h) != n_class) {
h = matrix(rep(h, n_class), ncol= n_class)
}
} else if (is.matrix(h)) {
if (nrow(h) == n_variable) {
if (ncol(h) == n_class) {
h =h
}
}
}
h
model
n <- 100
set.seed(1)
data <- data.frame(class = sample(c("classA", "classB"), n, TRUE),
bern = sample(LETTERS[1:2], n, TRUE),
cat  = sample(letters[1:3], n, TRUE),
logical = sample(c(TRUE,FALSE), n, TRUE),
norm = rnorm(n),
count = rpois(n, lambda = c(5,15)))
traindata = data.frame(data$norm, data$class)
ind <- sample(2, nrow(traindata), replace = T, prob = c(0.95, 0.05))
train_data <- traindata[ind == 1,]
test_data <- traindata[ind == 2,]
model <- naive_bayes(data.class ~ ., data = train_data,
usekernel = TRUE, kernel = "gaussian")
model
plot(model)
p
plot(model)
p <- predict(model, test_data, type = 'prob')
p
#########################
traindata = train_data
testdata = as.matrix(test_data[,-2])
h = c(0.4197, 0.3586)
# h= NULL
pdf = predict_nbkde(traindata=traindata, testdata=testdata, h=h)
pdf
p
subset.data = function(data) {
data = data
col.n = ncol(data)
class.name = as.character(unique(data[,col.n]))
class.name = noquote(class.name)
class.n = length(unique(data[,col.n]))
class.data = list()
for (i in 1:class.n) {
class.data[[i]] = data[data[col.n] ==  class.name[i], ]
}
return(class.data)
}
subset.data(data)
subset.data(traindata)
predict_nbkde = function(traindata, testdata, h) {
n_x = ncol(traindata) # no. of column in test data
n_variable = ncol(as.matrix(traindata[, -n_x]))  # no. of features
n_test = nrow(testdata) # no. of instances in testdata
n_train = nrow(traindata)
sort_traindata = dplyr::arrange(traindata, traindata[,n_x])
n_class = length(unique(traindata[,n_x])) # no. of class
class_name = unique(sort_traindata[, n_x]) #name of the class
prior = rep(NA, n_class)
for (k in 1:n_class) {
class = noquote(as.character(unique(traindata[,n_x])))
prior[k] = length(which(traindata[, n_x]  == class[k]))/n_train
}
length_prior = length(prior) # no. of prior
if (length_prior != n_class) {
stop("prior must have the same length as the number or class")
}
if (sum(prior) != 1) {
stop("sum of prior must equal to 1")
}
if (is.null(testdata)) {
testdata = traindata # the covariates
} else {
if (is.matrix(testdata) | is.data.frame(testdata)) {
testdata = as.data.frame(testdata)
}
else {
stop("testdata must be a matrix or data frame")
}
}
if (is.null(traindata)) {
stop("No train data")
} else {
if (is.matrix(traindata) | is.data.frame(traindata)) {
traindata = as.data.frame(traindata)
}
else {
stop("traindata must be a matrix or data frame")
}
}
subset_traindata = subset.data(sort_traindata)
if (is.null(h)) {
bw = 2^seq(from=-4, to= 2, by=0.1)
ind = list()
train = list()
test = list()
training_data = list()
testing_data = list()
loss = array(NA, dim=c(length(bw),  n_variable, n_class))
h = matrix(NA, ncol = n_class, nrow = n_variable)
for (k in 1:n_class) {
ind[[k]] <- sample(2, nrow(subset_traindata[[k]]),
replace = T, prob = c(0.8, 0.2))
train[[k]] <- subset_traindata[[k]][ind[[k]] == 1,]
test[[k]] <- subset_traindata[[k]][ind[[k]] == 2,]
for (w in 1:length(bw)) {
for (j in 1:n_variable) {
training_data[[k]] = train[[k]][,-n_x]
testing_data[[k]] = test[[k]][,-n_x]
loss[w, j, k] = logloss(h = bw[w], traindata = as.matrix(training_data[[k]])[,j],
testdata= as.matrix(testing_data[[k]])[,j])
}
}
}
for (k in 1:n_class) {
for (w in 1:length(bw)) {
for (j in 1:n_variable) {
h[j, k] =  bw[which(loss[, j, k] == min(loss[,j , k]))]
}
}
}
} else if (is.numeric(h)) {
if (length(h) != n_variable * n_class) {
stop("Number of bandwidth is invalid")
} else {
h = matrix(h, byrow = T, ncol = n_class)
}
} else if (is.matrix(h)) {
if (nrow(h) != n_variable) {
stop("Number of row for bandwidth is invalid. Number of column must equal to number of class")
}
if (ncol(h) != n_class) {
h = matrix(rep(h, n_class), ncol= n_class)
}
} else if (is.matrix(h)) {
if (nrow(h) == n_variable) {
if (ncol(h) == n_class) {
h =h
}
}
}
#   n_h = length(h) # no.of of bandwidth
#
#   h_matrix = matrix(h, ncol = n_variable)
#   # no. of column is the number of class
#   # no. of row is the number of variable
#
#   if (n_h == 1) {
#
#     h = matrix(rep(h, (n_variable * n_class)), ncol = n_class,nrow = n_variable, )
#
#   } else if (nrow(h_matrix) == 1) {
#
#   h = matrix(rep(h_matrix, n_class), ncol= n_class)
#
#   } else if (nrow(h_matrix) != n_class)  {
#
#   stop("Number of bandwidth is invalid")
#
#   } else if (is.numeric(h))  {
#
#   h = matrix(h, byrow = T, ncol = n_class )
#
#   } else {
#
#   h = h
#   }
# }
# h is a matrix of bandwidth with (C x P) dimension:
# C: no of class
# P: no of variable
data = list()
pdf = list()
for(k in 1:n_class) {
data[[k]]= subset_traindata[[k]][, -n_x]
pdf [[k]] = multiple.pdf(traindata = data[[k]],
testdata = testdata, h = h[,k])
}
# prior =  unlist(prior)
# return (c(pdf, prior))
prod_pdf = list()
for (k in 1:n_class) {
prod_pdf[[k]] = data.frame(unlist(pdf[[k]])) %>% mutate(Prod = Reduce(`*`, .))
}
prob_pdf = matrix(NA, nrow = n_test, ncol = n_class)
for (k in 1:n_class) {
prob_pdf[, k]  = prod_pdf[[k]]$Prod *  prior[k]
# this the probability for each pdf
# the row is the number of test point
# the column the number of class
}
sum_prob = as.matrix(apply(prob_pdf, 1, sum), byrow = T, nrow = n_test)
p_class = matrix(NA, nrow = n_test, ncol = n_class)
for (k in 1:n_class) {
for (i in 1:n_test) {
p_class[i,k ] = prob_pdf[i ,k]/sum_prob[i,]
}
}
p_class = as.data.frame(p_class)
colnames(p_class) <- class_name
h_matrix = h
# no. of column is number of class
# no. of row is number of variables
colnames(h_matrix) <- class_name
rownames(h_matrix) <- names(traindata[,-n_x])
result = list("conditional probability" = p_class,
"prior probability" = prior,
"bandwidth"= h_matrix)
return(result)
}
#########################
traindata = train_data
testdata = as.matrix(test_data[,-2])
h = c(0.4197, 0.3586)
# h= NULL
pdf = predict_nbkde(traindata=traindata, testdata=testdata, h=h)
pdf
p
subset.data(data)
subset.data(traindata)
